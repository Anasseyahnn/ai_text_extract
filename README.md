# Vision OCR Pro

![Banner](https://placehold.co/1200x300?text=Vision+OCR+Pro&font=montserrat)

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.31%2B-FF4B4B)](https://streamlit.io/)
[![Ollama](https://img.shields.io/badge/Ollama-0.1.29%2B-green)](https://ollama.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow)](LICENSE)

## üìë Description

**Vision OCR Pro** est une application web intuitive qui utilise l'intelligence artificielle pour extraire et structurer le texte √† partir d'images. Con√ßue pour maximiser la productivit√©, elle offre une alternative √©thique et locale aux services cloud d'OCR traditionnels.

### üîí Confidentialit√© et S√©curit√©

- **Traitement 100% local** - Aucun document n'est envoy√© sur des serveurs distants
- **Fonctionne hors connexion** - Id√©al pour les environnements s√©curis√©s
- **Contr√¥le total** - Vous ma√Ætrisez o√π vont vos donn√©es

## ‚ú® Fonctionnalit√©s

- **Reconnaissance pr√©cise** du texte dans les images
- **Pr√©servation de la structure** des documents (titres, paragraphes, tableaux)
- **Support multi-langues**
- **Options de formatage vari√©es** (texte structur√©, brut, tableau)
- **Interface utilisateur intuitive**
- **Historique des extractions**
- **Export des r√©sultats** en formats TXT et MD

## üì∏ Captures d'√©cran

![image](https://github.com/user-attachments/assets/6fcebdaa-d125-4072-bf1e-334e0cd9a224)


![image](https://github.com/user-attachments/assets/6c04481c-33d3-47ae-bde2-7d6577542ec4)



## üöÄ Installation

### Pr√©requis

- Python 3.9+
- [Ollama](https://ollama.com/) install√© et configur√©

### √âtapes d'installation

1. Clonez ce d√©p√¥t :
   ```bash
   git clone https://github.com/Anasseyahnn/vision-ocr-pro.git
   cd vision-ocr-pro
   ```

2. Cr√©ez un environnement virtuel (recommand√©) :
   ```bash
   python -m venv venv
   source venv/bin/activate  # Sur Windows: venv\Scripts\activate
   ```

3. Installez les d√©pendances :
   ```bash
   pip install -r requirements.txt
   ```

4. Assurez-vous que le mod√®le Gemma-3 est install√© dans Ollama :
   ```bash
   ollama pull gemma3:12b
   ```

## üíª Utilisation

1. D√©marrez l'application :
   ```bash
   streamlit run main1.py
   ```

2. Ouvrez votre navigateur et acc√©dez √† `http://localhost:8501`

3. T√©l√©chargez une image contenant du texte

4. Configurez les options d'extraction selon vos besoins

5. Cliquez sur "Extraire le texte" et obtenez les r√©sultats instantan√©ment

## üìù Guide d'utilisation

### Extraire du texte d'une image

1. Cliquez sur "Choisir l'image..." et s√©lectionnez une image (PNG, JPG, JPEG)
2. Configurez les options d'extraction :
   - **Format de sortie** : Texte structur√©, Brut, ou Tableau
   - **Langue principale** : Auto-d√©tection ou langue sp√©cifique
   - **D√©tection de tableaux** : Activ√©e ou d√©sactiv√©e
3. Cliquez sur "üîç Extraire le texte"
4. Consultez les r√©sultats dans les onglets "Texte format√©" ou "Texte brut"
5. Utilisez les boutons pour t√©l√©charger ou copier les r√©sultats

### Personnalisation des mod√®les

L'application prend en charge plusieurs mod√®les d'IA pour l'extraction de texte :
- **gemma3:12b** - Mod√®le par d√©faut, excellent √©quilibre entre pr√©cision et vitesse
- **gemma3:8b** - Plus rapide, id√©al pour les appareils moins puissants
- **llava:34b** - Plus pr√©cis pour les cas complexes, n√©cessite plus de ressources

## üõ†Ô∏è Fichiers du projet

```
vision-ocr-pro/
‚îú‚îÄ‚îÄ app.py                 # Application principale
‚îú‚îÄ‚îÄ requirements.txt       # D√©pendances Python
‚îú‚îÄ‚îÄ assets/                # Images et ressources statiques
‚îÇ   ‚îî‚îÄ‚îÄ placeholder.png    # Image de placeholder
‚îú‚îÄ‚îÄ README.md              # Documentation              
```

## üìä Performances

| Mod√®le | Temps moyen d'extraction | Pr√©cision | Consommation m√©moire |
|--------|--------------------------|-----------|----------------------|
| gemma3:12b | ~3.5 secondes | 98% | ~8 GB |
| gemma3:8b | ~2.1 secondes | 95% | ~5 GB |
| llava:34b | ~6.2 secondes | 99% | ~15 GB |

## üîß D√©pannage

### Probl√®mes courants

- **"Erreur de connexion √† Ollama"** : V√©rifiez qu'Ollama est bien lanc√© avec `ollama serve`
- **"Mod√®le non trouv√©"** : Assurez-vous d'avoir t√©l√©charg√© le mod√®le avec `ollama pull gemma3:12b`
- **"M√©moire insuffisante"** : Essayez un mod√®le plus l√©ger comme gemma3:8b

### Logs

Les logs sont disponibles dans le terminal o√π vous avez lanc√© l'application.

## ü§ù Contribution

Les contributions sont les bienvenues ! Voici comment vous pouvez contribuer :

1. Forkez ce d√©p√¥t
2. Cr√©ez une branche pour votre fonctionnalit√© (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Committez vos changements (`git commit -m 'Ajout d'une nouvelle fonctionnalit√©'`)
4. Poussez vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. Ouvrez une Pull Request

## üôè Remerciements

- [Streamlit](https://streamlit.io/) pour le framework d'interface utilisateur
- [Ollama](https://ollama.com/) pour l'ex√©cution locale des mod√®les d'IA
- [Google](https://blog.google/technology/ai/gemma-open-models/) pour le mod√®le open source Gemma-3

---

<div align="center">
  <p>D√©velopp√© avec ‚ù§Ô∏è par <a href="https://github.com/Anasseyahnn">Anasse Yahanan</a></p>
  <p>‚≠ê N'oubliez pas de mettre une √©toile √† ce projet si vous l'avez trouv√© utile! ‚≠ê</p>
</div>
